{
  "name": "Signify",
  "tagline": "Sign Language Recognition using Neural Networks",
  "body": "#**Welcome to Project Signify**\r\n\r\nProject Signify is a Computer Vision based Sign Language Recognition Device that relies on the Microsoft Kinect to identify Indian Sign Language Gestures. Our work classifies 3 gestures using Kinect for Windows (Global Feature Extraction) with the onboard RGB and Depth sensor cameras.\r\n\r\n\r\n***\r\n\r\n\r\n## **What is the significance of Signify?**\r\n\r\n\r\nThis system could find widespread use in schools where the hearing impaired generally require an interpreter for translation. It can find use in gatherings where the audience belongs to the hearing impaired community and needs to effectively communicate without third party intervention. It can be reverse engineered to learn/teach sign language based on the training gestures provided. \r\n\r\n\r\n***\r\n\r\n\r\n## **Project Phase**\r\n\r\nClassification of 3 gestures with **93.33% test accuracy** using both large margin classifiers and Neural Networks.\r\n\r\n([> **Click here for the White Paper**](https://drive.google.com/file/d/0BxcO5v3Bi80QeHFKdXE5cWY3ZUk/view?usp=sharing))\r\n\r\n\r\n***\r\n\r\n## **Future Scope**\r\n\r\nA single portable unit that runs on the Raspberry Pi. \r\nThe system can be enhanced to give better accuracy by taking more frames of video ( With increased computing resources). An array of gestures can be learned by the system with sufficient data. ( Ramped up data collection process). \r\n\r\n\r\n\r\n![Prototype](http://i.imgur.com/z3YEiK3.png?1)\r\n\r\n***\r\n\r\n## **The Development Team**\r\n\r\n@surajpaib , LinkedIn Profile: \r\n\r\n@nayan-mehta , LinkedIn Profile:\r\n\r\n@deepthitv , LinkedIn Profile:\r\n\r\n@KohliP , LinkedIn Profile:",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}